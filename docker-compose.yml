version: '3.10'

services:
  model_controller:
    container_name: model_controller
    image: registry.cn-beijing.aliyuncs.com/yuemengrui/ai:py311-ubuntu20.04-0.2
    command: [ "/bin/bash", "-c", "/workspace/ModelController/docker_run.sh" ]
    volumes:
      - ./ModelController:/workspace/ModelController
    ports:
      - "24620:24620"
    restart: unless-stopped
    networks:
      - lmcenternet
    stdin_open: true
    tty: true
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:24620/ai/health" ]
      interval: 60s
      start_period: 60s
      timeout: 10s
      retries: 5

  baichuan2_13b_server:
    container_name: model_worker_baichuan2_13b_server
    image: registry.cn-beijing.aliyuncs.com/yuemengrui/ai:pytorch2.1.2-cuda12.1-cudnn8-ubuntu20.04-py311-base
    command: [ "/bin/bash", "-c", "/workspace/ModelWorker/docker_run.sh" ]
    depends_on:
      model_controller:
        condition: service_healthy
        restart: true
    volumes:
      - ./ModelWorker:/workspace/ModelWorker
      - ./ModelWorker/configs/baichuan2_13b_configs.py:/workspace/ModelWorker/configs/configs.py
      - ./DATA/Models/Baichuan2-13B-Chat:/workspace/Models/Baichuan2-13B-Chat
    ports:
      - "24621:24621"
    restart: unless-stopped
    networks:
      - lmcenternet
    shm_size: "4gb"
    stdin_open: true
    tty: true
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:24621/ai/health" ]
      interval: 60s
      start_period: 120s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ "1" ]
              capabilities: [ gpu ]

  chatglm3_6b_32k_server:
    container_name: model_worker_chatglm3_6b_32k_server
    image: registry.cn-beijing.aliyuncs.com/yuemengrui/ai:pytorch2.1.2-cuda12.1-cudnn8-ubuntu20.04-py311-base
    command: [ "/bin/bash", "-c", "/workspace/ModelWorker/docker_run.sh" ]
    depends_on:
      model_controller:
        condition: service_healthy
        restart: true
    volumes:
      - ./ModelWorker:/workspace/ModelWorker
      - ./ModelWorker/configs/chatglm3_6b_configs.py:/workspace/ModelWorker/configs/configs.py
      - ./DATA/Models/chatglm3_6b_32k:/workspace/Models/chatglm3_6b_32k
    ports:
      - "24622:24621"
    restart: unless-stopped
    networks:
      - lmcenternet
    shm_size: "4gb"
    stdin_open: true
    tty: true
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:24621/ai/health" ]
      interval: 60s
      start_period: 120s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ "2" ]
              capabilities: [ gpu ]


  qwen2_7b_server:
    container_name: model_worker_qwen2_7b_server
    image: registry.cn-beijing.aliyuncs.com/yuemengrui/ai:pytorch2.1.2-cuda12.1-cudnn8-ubuntu20.04-py311-base
    command: [ "/bin/bash", "-c", "/workspace/ModelWorker/docker_run.sh" ]
    depends_on:
      model_controller:
        condition: service_healthy
        restart: true
    volumes:
      - ./ModelWorker:/workspace/ModelWorker
      - ./ModelWorker/configs/qwen2_7b_configs.py:/workspace/ModelWorker/configs/configs.py
      - ./DATA/Models/Qwen1.5-7B-Chat:/workspace/Models/Qwen1.5-7B-Chat
    ports:
      - "24623:24621"
    restart: unless-stopped
    networks:
      - lmcenternet
    shm_size: "4gb"
    stdin_open: true
    tty: true
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:24621/ai/health" ]
      interval: 60s
      start_period: 120s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ "3" ]
              capabilities: [ gpu ]



networks:
  lmcenternet:
    driver: bridge
    name: lmcenternet
